{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3-2CeFd6e3g"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
<<<<<<< HEAD
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfRkJJM26-M7",
    "outputId": "2709b6ae-0de7-476f-ce6c-4a56f84f91a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Zuzka\\git\\02456_DL-sketch_features_VAE\\vae\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "log_interval = 100\n",
    "epochs = 10\n",
    "root = os.getcwd()\n",
    "print(f\"Current working directory: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yj5MFq1c5CPd"
   },
   "outputs": [],
   "source": [
    "def load_mnist_binarized(root):\n",
    "    datapath = os.path.join(root, 'bin-mnist')\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath)\n",
    "    dataset = os.path.join(datapath, \"mnist.pkl.gz\")\n",
    "\n",
    "    if not os.path.isfile(dataset):\n",
    "\n",
    "        datafiles = {\n",
    "            \"train\": \"http://www.cs.toronto.edu/~larocheh/public/\"\n",
    "                     \"datasets/binarized_mnist/binarized_mnist_train.amat\",\n",
    "            \"valid\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
    "                     \"binarized_mnist/binarized_mnist_valid.amat\",\n",
    "            \"test\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
    "                    \"binarized_mnist/binarized_mnist_test.amat\"\n",
    "        }\n",
    "        datasplits = {}\n",
    "        for split in datafiles.keys():\n",
    "            print(\"Downloading %s data...\" % (split))\n",
    "            datasplits[split] = np.loadtxt(urlretrieve(datafiles[split])[0])\n",
    "\n",
    "        pkl.dump([datasplits['train'], datasplits['valid'], datasplits['test']], open(dataset, \"wb\"))\n",
    "\n",
    "    x_train, x_valid, x_test = pkl.load(open(dataset, \"rb\"))\n",
    "    return x_train, x_valid, x_test\n",
    "\n",
    "\n",
    "class BinMNIST(Dataset):\n",
    "    \"\"\"Binary MNIST dataset\"\"\"\n",
    "\n",
    "    def __init__(self, data, device='cpu', transform=None):\n",
    "        h, w, c = 28, 28, 1\n",
    "        self.device = device\n",
    "        self.data = torch.tensor(data, dtype=torch.float).view(-1, c, h, w)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample.to(self.device)\n",
    "\n",
    "\n",
    "def get_binmnist_datasets(root, device='cpu'):\n",
    "    x_train, x_valid, x_test = load_mnist_binarized(root)\n",
    "    x_train = np.append(x_train, x_valid, axis=0)  # https://github.com/casperkaae/LVAE/blob/master/run_models.py (line 401)\n",
    "    return BinMNIST(x_train, device=device), BinMNIST(x_test, device=device), BinMNIST(x_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPaACQn06BbW"
   },
   "outputs": [],
   "source": [
    "ngf = 32#64\n",
    "ndf = 32#64\n",
    "nc = 1\n",
    "h_dim= 256#1024\n",
    "\n",
    "\n",
    "class conv_VAE(nn.Module):\n",
    "    def __init__(self, nz=32):\n",
    "        super(conv_VAE, self).__init__()\n",
    "        \n",
    "        self.have_cuda = True\n",
    "        self.nz = nz\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # input is (nc) x 28 x 28\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 14 x 14\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, h_dim, 4, 1, 0, bias=False),\n",
    "            # nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( h_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     nc, 4, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(ngf),\n",
    "            # nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            # nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            # nn.Tanh()\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, 512)\n",
    "        self.fc21 = nn.Linear(512, nz)\n",
    "        self.fc22 = nn.Linear(512, nz)\n",
    "\n",
    "        self.fc3 = nn.Linear(nz, 512)\n",
    "        self.fc4 = nn.Linear(512, h_dim)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        conv = self.encoder(x);\n",
    "        # print(\"encode conv\", conv.size())\n",
    "        h1 = self.fc1(conv.view(-1, h_dim))\n",
    "        # print(\"encode h1\", h1.size())\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        deconv_input = self.fc4(h3)\n",
    "        # print(\"deconv_input\", deconv_input.size())\n",
    "        deconv_input = deconv_input.view(-1, h_dim, 1, 1)\n",
    "        # print(\"deconv_input\", deconv_input.size())\n",
    "        return self.decoder(deconv_input)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "#         std = logvar.mul(0.5).exp_()\n",
    "#         if self.have_cuda:\n",
    "#             eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "#         else:\n",
    "#             eps = torch.FloatTensor(std.size()).normal_()\n",
    "#         eps = Variable(eps)\n",
    "#         return eps.mul(std).add_(mu)\n",
    "    \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x\", x.size())\n",
    "        mu, logvar = self.encode(x)\n",
    "        # print(\"mu, logvar\", mu.size(), logvar.size())\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        # print(\"z\", z.size())\n",
    "        decoded = self.decode(z)\n",
    "        # print(\"decoded\", decoded.size())\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ3GUPZC1YGp"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = get_binmnist_datasets(root)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n",
    "test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ3GUPZC1YGp"
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)\n",
    "\n",
    "    return BCE + KLD, kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ3GUPZC1YGp"
   },
   "outputs": [],
   "source": [
    "model = conv_VAE().to(device)\n",
    "# model = VAE().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_loss_all = []\n",
    "test_loss_all = []\n",
    "kl_loss_train = []\n",
    "kl_loss_test = []\n",
    "test_loss_all = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    batch_idx = 0\n",
    "    batch_kl = []\n",
    "    batch_elbo = []\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_kl.append(kld.item())\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "            \n",
    "        batch_idx = batch_idx + 1\n",
    "\n",
    "    kl_loss_train.append(np.mean(batch_kl))\n",
    "\n",
    "    train_mean = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_mean))\n",
    "    train_loss_all.append(train_mean)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    show_img = False\n",
    "    datapath = os.path.join(root, 'results')\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    batch_kl = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader: \n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            batch_kl.append(kld.item())\n",
    "\n",
    "            # if i == 0:\n",
    "            #     n = min(data.size(0), 8)\n",
    "            #     comparison = torch.cat([data[:n],\n",
    "            #                           recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "            #     save_image(comparison.cpu(),\n",
    "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "     \n",
    "            recon_batch = recon_batch.to(\"cpu\")\n",
    "\n",
    "            if show_img:\n",
    "                # Show input digits\n",
    "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
    "                for i, ax in enumerate(axarr.flat):\n",
    "                    ax.imshow(data[i].view(28, 28), cmap=\"binary_r\")\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle('Inputs')\n",
    "                plt.show()\n",
    "\n",
    "                # Show reconstructions\n",
    "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
    "                for i, ax in enumerate(axarr.flat):\n",
    "                    ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle('Reconstructions')\n",
    "                plt.show()\n",
    "\n",
    "                # Show latent space samples        \n",
    "                with torch.no_grad():\n",
    "                    sample = torch.randn(64, 20).to(device)\n",
    "                    sample = model.decode(sample).cpu()\n",
    "                    f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
    "                    for i, ax in enumerate(axarr.flat):\n",
    "                        ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n",
    "                        ax.axis('off')\n",
    "                    plt.suptitle('Latent space')\n",
    "                    plt.show()\n",
    "                show_img = False\n",
    "\n",
    "    kl_loss_test.append(np.mean(batch_kl))\n",
    "\n",
    "    test_mean = test_loss / len(test_loader.dataset)\n",
    "    test_loss_all.append(test_mean)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nomD1zG6g_kp"
   },
   "outputs": [],
   "source": [
    "def plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test):\n",
    "        # Overall loss (ELBO)\n",
    "        plt.plot(epoch_list, train_loss_all, color=\"blue\")\n",
    "        plt.plot(epoch_list, test_loss_all, color=\"green\", linestyle=\"--\")\n",
    "        plt.legend(['Training', 'Testing'])\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "        # KL loss\n",
    "        plt.plot(epoch_list, kl_loss_train, color=\"blue\")\n",
    "        plt.plot(epoch_list, kl_loss_test, color=\"green\", linestyle=\"--\")\n",
    "        plt.legend(['Training', 'Testing'])\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('KL loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
=======
      "name": "Copy of Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3-2CeFd6e3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle as pkl\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
>>>>>>> master
    },
    "colab_type": "code",
    "id": "1EoPFjWr95NZ",
    "outputId": "1266447a-9fdf-4b35-9537-aca8eefbba70"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Using a target size (torch.Size([64, 784])) that is different to the input size (torch.Size([64, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
=======
      "cell_type": "code",
      "metadata": {
        "id": "YfRkJJM26-M7",
        "colab_type": "code",
        "outputId": "12282aef-0a8e-44e8-9209-089d63d4400d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "log_interval = 100\n",
        "epochs = 10\n",
        "root = os.getcwd()\n",
        "print(f\"Current working directory: {root}\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n"
          ],
          "name": "stdout"
        }
      ]
>>>>>>> master
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 941.092285\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 208.259766\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 176.961090\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 155.093811\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 138.906830\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 128.275055\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 127.230171\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 115.881401\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 112.358177\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 117.054794\n"
     ]
    },
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Using a target size (torch.Size([32, 784])) that is different to the input size (torch.Size([32, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 149.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Using a target size (torch.Size([16, 784])) that is different to the input size (torch.Size([16, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 115.5637\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 113.852547\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 117.271118\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 113.893768\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 112.169838\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 115.882965\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 108.651558\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 108.682968\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 107.061821\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 107.970856\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 102.483353\n",
      "====> Epoch: 2 Average loss: 109.2394\n",
      "====> Test set loss: 104.2716\n",
      "################################################################\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 101.541328\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 107.183945\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 100.538986\n"
     ]
=======
      "cell_type": "code",
      "metadata": {
        "id": "UPaACQn06BbW",
        "colab_type": "code",
        "outputId": "b0101c5e-dacd-41b7-efa7-821728b586e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x_train, x_valid, x_test = get_binmnist_datasets(root)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n",
        "test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train data...\n",
            "Downloading valid data...\n",
            "Downloading test data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ3GUPZC1YGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loss_all = []\n",
        "test_loss_all = []\n",
        "kl_loss_train = []\n",
        "kl_loss_test = []\n",
        "test_loss_all = []\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)\n",
        "\n",
        "    return BCE + KLD, kl.mean()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    batch_idx = 0\n",
        "    batch_kl = []\n",
        "    batch_elbo = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_kl.append(kld.item())\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "            \n",
        "        batch_idx = batch_idx + 1\n",
        "\n",
        "    kl_loss_train.append(np.mean(batch_kl))\n",
        "\n",
        "    train_mean = train_loss / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_mean))\n",
        "    train_loss_all.append(train_mean)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    show_img = False\n",
        "    save_img = True\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    batch_kl = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader: \n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            batch_kl.append(kld.item())\n",
        "\n",
        "            if data.shape[0] == batch_size:\n",
        "                out_batch_comparison(data, recon_batch, epoch)\n",
        "\n",
        "            # if epoch == 1:\n",
        "            #     n = min(data.size(0), 8)\n",
        "            #     comparison = torch.cat([data[:n],\n",
        "            #                           recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "            #     save_image(comparison.cpu(),\n",
        "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "     \n",
        "            recon_batch = recon_batch.to(\"cpu\")\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "\n",
        "            if show_img:\n",
        "                # Show input digits\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(data[i].cpu().view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Inputs')\n",
        "                plt.show()\n",
        "\n",
        "                # Show reconstructions\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Reconstructions')\n",
        "                plt.show()\n",
        "\n",
        "                # Show latent space samples        \n",
        "                with torch.no_grad():\n",
        "                    # sample = torch.randn(64, 20).to(device)\n",
        "                    # sample = model.decode(sample).cpu()\n",
        "                    f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                    for i, ax in enumerate(axarr.flat):\n",
        "                        ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n",
        "                        ax.axis('off')\n",
        "                    plt.suptitle('Latent space')\n",
        "                    plt.show()\n",
        "                show_img = False\n",
        "\n",
        "            if save_img:\n",
        "                # out_batch_image(data, \"test_input\", epoch)\n",
        "                # out_batch_image(recon_batch, \"test_output\", epoch)\n",
        "                out_batch_image(sample, \"latent_sample\", epoch)\n",
        "\n",
        "    kl_loss_test.append(np.mean(batch_kl))\n",
        "\n",
        "    test_mean = test_loss / len(test_loader.dataset)\n",
        "    test_loss_all.append(test_mean)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nomD1zG6g_kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test):\n",
        "        # Overall loss (ELBO)\n",
        "        plt.plot(epoch_list, train_loss_all, color=\"blue\")\n",
        "        plt.plot(epoch_list, test_loss_all, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "        # KL loss\n",
        "        plt.plot(epoch_list, kl_loss_train, color=\"blue\")\n",
        "        plt.plot(epoch_list, kl_loss_test, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('KL loss')\n",
        "        plt.show()\n",
        "\n",
        "def out_batch_image(data, prefix, epoch):\n",
        "    save_image(data.view(-1, 1, 28, 28),\n",
        "                       f'results/{prefix}_{epoch}.png')\n",
        "\n",
        "def out_batch_comparison(input, recon, epoch):\n",
        "    comparison = torch.cat([input, recon.view(-1, 1, 28, 28)])\n",
        "    out_batch_image(comparison, \"comparison\", epoch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EoPFjWr95NZ",
        "colab_type": "code",
        "outputId": "2c39a6ef-0453-4ac1-c6fd-20ea95cfe70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    datapath = os.path.join(root, 'results')\n",
        "    if os.path.exists(datapath):\n",
        "        shutil.rmtree('results')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "\n",
        "    epoch_list = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        epoch_list.append(epoch)\n",
        "        if epoch == 1:\n",
        "            continue\n",
        "\n",
        "        plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test)\n",
        "        print(\"################################################################\")\n",
        "            \n",
        "            # save_image(sample.view(64, 1, 28, 28),\n",
        "            #            'results/sample_' + str(epoch) + '.png')\n",
        "\n",
        "        #clear_output(wait=True)\n"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.070312\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 188.820602\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 166.234116\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 150.769318\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 134.058929\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 126.164001\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 126.026207\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 132.819214\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 120.640434\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 123.504448\n",
            "====> Epoch: 1 Average loss: 146.3072\n",
            "====> Test set loss: 117.0949\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 115.720528\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 109.011238\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 109.281021\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 107.114166\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 116.229439\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 112.514679\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 113.101593\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 109.169449\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 110.944824\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 112.560310\n",
            "====> Epoch: 2 Average loss: 113.4724\n",
            "====> Test set loss: 109.7257\n",
            "################################################################\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 108.039612\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 102.284187\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 113.693985\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 105.331863\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 104.129097\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 111.674179\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 102.161522\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 102.980896\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 105.423004\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 108.572502\n",
            "====> Epoch: 3 Average loss: 108.8341\n",
            "====> Test set loss: 106.9986\n",
            "################################################################\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 98.602364\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 106.427620\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 104.742020\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 109.978226\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 102.461998\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 110.599007\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.571968\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 106.477142\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 104.922928\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 105.647911\n",
            "====> Epoch: 4 Average loss: 106.6213\n",
            "====> Test set loss: 105.5940\n",
            "################################################################\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 101.780457\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 101.017365\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 105.842751\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 102.734421\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 103.482651\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 109.596062\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 103.718193\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 102.007439\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 102.156311\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 102.382927\n",
            "====> Epoch: 5 Average loss: 105.3213\n",
            "====> Test set loss: 104.5152\n",
            "################################################################\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 102.200981\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 104.008148\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 102.682175\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 100.675186\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 102.597000\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 108.728546\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 106.433060\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 102.717110\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.298691\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 102.531799\n",
            "====> Epoch: 6 Average loss: 104.3426\n",
            "====> Test set loss: 103.7401\n",
            "################################################################\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.249390\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 104.241005\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 102.094086\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 103.674927\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 105.672150\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 99.196434\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 108.604019\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 103.656448\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 102.190308\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 99.546432\n",
            "====> Epoch: 7 Average loss: 103.6449\n",
            "====> Test set loss: 103.4695\n",
            "################################################################\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 103.919846\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 102.796333\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 99.871063\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 103.057617\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 103.710281\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 105.550186\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.288086\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 107.638184\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 102.750626\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 109.976746\n",
            "====> Epoch: 8 Average loss: 103.0298\n",
            "====> Test set loss: 102.8133\n",
            "################################################################\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 100.546707\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 100.466736\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 102.159805\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 107.130592\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 103.508575\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 104.137741\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 98.651031\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 107.307663\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 94.567932\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 105.436249\n",
            "====> Epoch: 9 Average loss: 102.5828\n",
            "====> Test set loss: 102.6125\n",
            "################################################################\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.354752\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 102.830627\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 102.478783\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 98.832939\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 103.542816\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 100.718506\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.592819\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 106.993439\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 101.331352\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 103.253922\n",
            "====> Epoch: 10 Average loss: 102.1812\n",
            "====> Test set loss: 102.2497\n",
            "################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JyyoTgeTpMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "#                     help='input batch size for training (default: 128)')\n",
        "# parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "#                     help='number of epochs to train (default: 10)')\n",
        "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "#                     help='enables CUDA training')\n",
        "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "#                     help='how many batches to wait before logging training status')\n",
        "# args = parser.parse_args()\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "# torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
>>>>>>> master
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    epoch_list = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        epoch_list.append(epoch)\n",
    "        if epoch == 1:\n",
    "            continue\n",
    "\n",
    "        # plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test)\n",
    "        print(\"################################################################\")\n",
    "            \n",
    "            # save_image(sample.view(64, 1, 28, 28),\n",
    "            #            'results/sample_' + str(epoch) + '.png')\n",
    "\n",
    "        #clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
