{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3-2CeFd6e3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle as pkl\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRkJJM26-M7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6031189f-4a13-4c52-b8a3-734b95015b37"
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "log_interval = 100\n",
        "epochs = 10\n",
        "root = os.getcwd()\n",
        "print(f\"Current working directory: {root}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5MFq1c5CPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_binarized(root):\n",
        "    datapath = os.path.join(root, 'bin-mnist')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "    dataset = os.path.join(datapath, \"mnist.pkl.gz\")\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "\n",
        "        datafiles = {\n",
        "            \"train\": \"http://www.cs.toronto.edu/~larocheh/public/\"\n",
        "                     \"datasets/binarized_mnist/binarized_mnist_train.amat\",\n",
        "            \"valid\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                     \"binarized_mnist/binarized_mnist_valid.amat\",\n",
        "            \"test\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                    \"binarized_mnist/binarized_mnist_test.amat\"\n",
        "        }\n",
        "        datasplits = {}\n",
        "        for split in datafiles.keys():\n",
        "            print(\"Downloading %s data...\" % (split))\n",
        "            datasplits[split] = np.loadtxt(urlretrieve(datafiles[split])[0])\n",
        "\n",
        "        pkl.dump([datasplits['train'], datasplits['valid'], datasplits['test']], open(dataset, \"wb\"))\n",
        "\n",
        "    x_train, x_valid, x_test = pkl.load(open(dataset, \"rb\"))\n",
        "    return x_train, x_valid, x_test\n",
        "\n",
        "\n",
        "class BinMNIST(Dataset):\n",
        "    \"\"\"Binary MNIST dataset\"\"\"\n",
        "\n",
        "    def __init__(self, data, device='cpu', transform=None):\n",
        "        h, w, c = 28, 28, 1\n",
        "        self.device = device\n",
        "        self.data = torch.tensor(data, dtype=torch.float).view(-1, c, h, w)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample.to(self.device)\n",
        "\n",
        "\n",
        "def get_binmnist_datasets(root, device='cpu'):\n",
        "    x_train, x_valid, x_test = load_mnist_binarized(root)\n",
        "    x_train = np.append(x_train, x_valid, axis=0)  # https://github.com/casperkaae/LVAE/blob/master/run_models.py (line 401)\n",
        "    return BinMNIST(x_train, device=device), BinMNIST(x_test, device=device), BinMNIST(x_test, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPaACQn06BbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_valid, x_test = get_binmnist_datasets(root)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n",
        "test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ3GUPZC1YGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "#                     help='input batch size for training (default: 128)')\n",
        "# parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "#                     help='number of epochs to train (default: 10)')\n",
        "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "#                     help='enables CUDA training')\n",
        "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "#                     help='how many batches to wait before logging training status')\n",
        "# args = parser.parse_args()\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "# torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loss_all = []\n",
        "test_loss_all = []\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    batch_idx = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "            \n",
        "        batch_idx = batch_idx + 1\n",
        "\n",
        "    train_mean = train_loss / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_mean))\n",
        "    train_loss_all.append(train_mean)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    datapath = os.path.join(root, 'results')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            # if i == 0:\n",
        "            #     n = min(data.size(0), 8)\n",
        "            #     comparison = torch.cat([data[:n],\n",
        "            #                           recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "            #     save_image(comparison.cpu(),\n",
        "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_mean = test_loss / len(test_loader.dataset)\n",
        "    test_loss_all.append(test_mean)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EoPFjWr95NZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "04435559-7747-48de-9683-ba1921d40b51"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       'results/sample_' + str(epoch) + '.png')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 552.560120\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 195.871613\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 156.520309\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 145.134445\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 136.538651\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 130.407074\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 126.422974\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 117.254539\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 113.629349\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 121.982834\n",
            "====> Epoch: 1 Average loss: 145.9234\n",
            "====> Test set loss: 117.0736\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 111.100090\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 116.326805\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 112.259193\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 111.494766\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 109.622299\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 107.448357\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 116.335220\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 110.785126\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 110.389915\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 118.647133\n",
            "====> Epoch: 2 Average loss: 113.3467\n",
            "====> Test set loss: 109.2381\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 115.791435\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 105.952988\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 111.672974\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 108.580276\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.780823\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 107.419289\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.435013\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 98.929184\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}