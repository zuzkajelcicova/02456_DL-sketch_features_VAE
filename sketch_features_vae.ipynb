{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3-2CeFd6e3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle as pkl\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRkJJM26-M7",
        "colab_type": "code",
        "outputId": "12282aef-0a8e-44e8-9209-089d63d4400d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "log_interval = 100\n",
        "epochs = 10\n",
        "root = os.getcwd()\n",
        "print(f\"Current working directory: {root}\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5MFq1c5CPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist_binarized(root):\n",
        "    datapath = os.path.join(root, 'bin-mnist')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "    dataset = os.path.join(datapath, \"mnist.pkl.gz\")\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "\n",
        "        datafiles = {\n",
        "            \"train\": \"http://www.cs.toronto.edu/~larocheh/public/\"\n",
        "                     \"datasets/binarized_mnist/binarized_mnist_train.amat\",\n",
        "            \"valid\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                     \"binarized_mnist/binarized_mnist_valid.amat\",\n",
        "            \"test\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                    \"binarized_mnist/binarized_mnist_test.amat\"\n",
        "        }\n",
        "        datasplits = {}\n",
        "        for split in datafiles.keys():\n",
        "            print(\"Downloading %s data...\" % (split))\n",
        "            datasplits[split] = np.loadtxt(urlretrieve(datafiles[split])[0])\n",
        "\n",
        "        pkl.dump([datasplits['train'], datasplits['valid'], datasplits['test']], open(dataset, \"wb\"))\n",
        "\n",
        "    x_train, x_valid, x_test = pkl.load(open(dataset, \"rb\"))\n",
        "    return x_train, x_valid, x_test\n",
        "\n",
        "\n",
        "class BinMNIST(Dataset):\n",
        "    \"\"\"Binary MNIST dataset\"\"\"\n",
        "\n",
        "    def __init__(self, data, device='cpu', transform=None):\n",
        "        h, w, c = 28, 28, 1\n",
        "        self.device = device\n",
        "        self.data = torch.tensor(data, dtype=torch.float).view(-1, c, h, w)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample.to(self.device)\n",
        "\n",
        "\n",
        "def get_binmnist_datasets(root, device='cpu'):\n",
        "    x_train, x_valid, x_test = load_mnist_binarized(root)\n",
        "    x_train = np.append(x_train, x_valid, axis=0)  # https://github.com/casperkaae/LVAE/blob/master/run_models.py (line 401)\n",
        "    return BinMNIST(x_train, device=device), BinMNIST(x_test, device=device), BinMNIST(x_test, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPaACQn06BbW",
        "colab_type": "code",
        "outputId": "b0101c5e-dacd-41b7-efa7-821728b586e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x_train, x_valid, x_test = get_binmnist_datasets(root)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n",
        "test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train data...\n",
            "Downloading valid data...\n",
            "Downloading test data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ3GUPZC1YGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loss_all = []\n",
        "test_loss_all = []\n",
        "kl_loss_train = []\n",
        "kl_loss_test = []\n",
        "test_loss_all = []\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)\n",
        "\n",
        "    return BCE + KLD, kl.mean()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    batch_idx = 0\n",
        "    batch_kl = []\n",
        "    batch_elbo = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_kl.append(kld.item())\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "            \n",
        "        batch_idx = batch_idx + 1\n",
        "\n",
        "    kl_loss_train.append(np.mean(batch_kl))\n",
        "\n",
        "    train_mean = train_loss / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_mean))\n",
        "    train_loss_all.append(train_mean)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    show_img = False\n",
        "    save_img = True\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    batch_kl = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader: \n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            batch_kl.append(kld.item())\n",
        "\n",
        "            if data.shape[0] == batch_size:\n",
        "                out_batch_comparison(data, recon_batch, epoch)\n",
        "\n",
        "            # if epoch == 1:\n",
        "            #     n = min(data.size(0), 8)\n",
        "            #     comparison = torch.cat([data[:n],\n",
        "            #                           recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "            #     save_image(comparison.cpu(),\n",
        "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "     \n",
        "            recon_batch = recon_batch.to(\"cpu\")\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "\n",
        "            if show_img:\n",
        "                # Show input digits\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(data[i].cpu().view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Inputs')\n",
        "                plt.show()\n",
        "\n",
        "                # Show reconstructions\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Reconstructions')\n",
        "                plt.show()\n",
        "\n",
        "                # Show latent space samples        \n",
        "                with torch.no_grad():\n",
        "                    # sample = torch.randn(64, 20).to(device)\n",
        "                    # sample = model.decode(sample).cpu()\n",
        "                    f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                    for i, ax in enumerate(axarr.flat):\n",
        "                        ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n",
        "                        ax.axis('off')\n",
        "                    plt.suptitle('Latent space')\n",
        "                    plt.show()\n",
        "                show_img = False\n",
        "\n",
        "            if save_img:\n",
        "                # out_batch_image(data, \"test_input\", epoch)\n",
        "                # out_batch_image(recon_batch, \"test_output\", epoch)\n",
        "                out_batch_image(sample, \"latent_sample\", epoch)\n",
        "\n",
        "    kl_loss_test.append(np.mean(batch_kl))\n",
        "\n",
        "    test_mean = test_loss / len(test_loader.dataset)\n",
        "    test_loss_all.append(test_mean)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nomD1zG6g_kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test):\n",
        "        # Overall loss (ELBO)\n",
        "        plt.plot(epoch_list, train_loss_all, color=\"blue\")\n",
        "        plt.plot(epoch_list, test_loss_all, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "        # KL loss\n",
        "        plt.plot(epoch_list, kl_loss_train, color=\"blue\")\n",
        "        plt.plot(epoch_list, kl_loss_test, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('KL loss')\n",
        "        plt.show()\n",
        "\n",
        "def out_batch_image(data, prefix, epoch):\n",
        "    save_image(data.view(-1, 1, 28, 28),\n",
        "                       f'results/{prefix}_{epoch}.png')\n",
        "\n",
        "def out_batch_comparison(input, recon, epoch):\n",
        "    comparison = torch.cat([input, recon.view(-1, 1, 28, 28)])\n",
        "    out_batch_image(comparison, \"comparison\", epoch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EoPFjWr95NZ",
        "colab_type": "code",
        "outputId": "2c39a6ef-0453-4ac1-c6fd-20ea95cfe70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    datapath = os.path.join(root, 'results')\n",
        "    if os.path.exists(datapath):\n",
        "        shutil.rmtree('results')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "\n",
        "    epoch_list = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        epoch_list.append(epoch)\n",
        "        if epoch == 1:\n",
        "            continue\n",
        "\n",
        "        plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test)\n",
        "        print(\"################################################################\")\n",
        "            \n",
        "            # save_image(sample.view(64, 1, 28, 28),\n",
        "            #            'results/sample_' + str(epoch) + '.png')\n",
        "\n",
        "        #clear_output(wait=True)\n"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.070312\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 188.820602\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 166.234116\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 150.769318\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 134.058929\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 126.164001\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 126.026207\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 132.819214\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 120.640434\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 123.504448\n",
            "====> Epoch: 1 Average loss: 146.3072\n",
            "====> Test set loss: 117.0949\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 115.720528\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 109.011238\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 109.281021\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 107.114166\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 116.229439\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 112.514679\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 113.101593\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 109.169449\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 110.944824\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 112.560310\n",
            "====> Epoch: 2 Average loss: 113.4724\n",
            "====> Test set loss: 109.7257\n",
            "################################################################\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 108.039612\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 102.284187\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 113.693985\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 105.331863\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 104.129097\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 111.674179\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 102.161522\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 102.980896\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 105.423004\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 108.572502\n",
            "====> Epoch: 3 Average loss: 108.8341\n",
            "====> Test set loss: 106.9986\n",
            "################################################################\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 98.602364\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 106.427620\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 104.742020\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 109.978226\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 102.461998\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 110.599007\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.571968\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 106.477142\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 104.922928\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 105.647911\n",
            "====> Epoch: 4 Average loss: 106.6213\n",
            "====> Test set loss: 105.5940\n",
            "################################################################\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 101.780457\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 101.017365\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 105.842751\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 102.734421\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 103.482651\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 109.596062\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 103.718193\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 102.007439\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 102.156311\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 102.382927\n",
            "====> Epoch: 5 Average loss: 105.3213\n",
            "====> Test set loss: 104.5152\n",
            "################################################################\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 102.200981\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 104.008148\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 102.682175\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 100.675186\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 102.597000\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 108.728546\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 106.433060\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 102.717110\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.298691\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 102.531799\n",
            "====> Epoch: 6 Average loss: 104.3426\n",
            "====> Test set loss: 103.7401\n",
            "################################################################\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.249390\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 104.241005\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 102.094086\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 103.674927\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 105.672150\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 99.196434\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 108.604019\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 103.656448\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 102.190308\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 99.546432\n",
            "====> Epoch: 7 Average loss: 103.6449\n",
            "====> Test set loss: 103.4695\n",
            "################################################################\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 103.919846\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 102.796333\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 99.871063\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 103.057617\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 103.710281\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 105.550186\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.288086\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 107.638184\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 102.750626\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 109.976746\n",
            "====> Epoch: 8 Average loss: 103.0298\n",
            "====> Test set loss: 102.8133\n",
            "################################################################\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 100.546707\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 100.466736\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 102.159805\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 107.130592\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 103.508575\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 104.137741\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 98.651031\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 107.307663\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 94.567932\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 105.436249\n",
            "====> Epoch: 9 Average loss: 102.5828\n",
            "====> Test set loss: 102.6125\n",
            "################################################################\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.354752\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 102.830627\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 102.478783\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 98.832939\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 103.542816\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 100.718506\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.592819\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 106.993439\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 101.331352\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 103.253922\n",
            "====> Epoch: 10 Average loss: 102.1812\n",
            "====> Test set loss: 102.2497\n",
            "################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JyyoTgeTpMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "#                     help='input batch size for training (default: 128)')\n",
        "# parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "#                     help='number of epochs to train (default: 10)')\n",
        "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "#                     help='enables CUDA training')\n",
        "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "#                     help='how many batches to wait before logging training status')\n",
        "# args = parser.parse_args()\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "# torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}