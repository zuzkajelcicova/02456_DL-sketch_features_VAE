{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "newVAE_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z3-2CeFd6e3g",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from urllib.request import urlretrieve\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfRkJJM26-M7",
        "outputId": "b2535dc3-48f9-4401-ece4-675c29d7e545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "log_interval = 100\n",
        "epochs = 10\n",
        "root = os.getcwd()\n",
        "print(f\"Current working directory: {root}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yj5MFq1c5CPd",
        "colab": {}
      },
      "source": [
        "def load_mnist_binarized(root):\n",
        "    datapath = os.path.join(root, 'bin-mnist')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "    dataset = os.path.join(datapath, \"mnist.pkl.gz\")\n",
        "\n",
        "    if not os.path.isfile(dataset):\n",
        "\n",
        "        datafiles = {\n",
        "            \"train\": \"http://www.cs.toronto.edu/~larocheh/public/\"\n",
        "                     \"datasets/binarized_mnist/binarized_mnist_train.amat\",\n",
        "            \"valid\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                     \"binarized_mnist/binarized_mnist_valid.amat\",\n",
        "            \"test\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n",
        "                    \"binarized_mnist/binarized_mnist_test.amat\"\n",
        "        }\n",
        "        datasplits = {}\n",
        "        for split in datafiles.keys():\n",
        "            print(\"Downloading %s data...\" % (split))\n",
        "            datasplits[split] = np.loadtxt(urlretrieve(datafiles[split])[0])\n",
        "\n",
        "        pkl.dump([datasplits['train'], datasplits['valid'], datasplits['test']], open(dataset, \"wb\"))\n",
        "\n",
        "    x_train, x_valid, x_test = pkl.load(open(dataset, \"rb\"))\n",
        "    return x_train, x_valid, x_test\n",
        "\n",
        "\n",
        "class BinMNIST(Dataset):\n",
        "    \"\"\"Binary MNIST dataset\"\"\"\n",
        "\n",
        "    def __init__(self, data, device='cpu', transform=None):\n",
        "        h, w, c = 28, 28, 1\n",
        "        self.device = device\n",
        "        self.data = torch.tensor(data, dtype=torch.float).view(-1, c, h, w)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample.to(self.device)\n",
        "\n",
        "\n",
        "def get_binmnist_datasets(root, device='cpu'):\n",
        "    x_train, x_valid, x_test = load_mnist_binarized(root)\n",
        "    x_train = np.append(x_train, x_valid, axis=0)  # https://github.com/casperkaae/LVAE/blob/master/run_models.py (line 401)\n",
        "    return BinMNIST(x_train, device=device), BinMNIST(x_test, device=device), BinMNIST(x_test, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPaACQn06BbW",
        "colab": {}
      },
      "source": [
        "ngf = 32 #64\n",
        "ndf = 32 #64\n",
        "nc = 1\n",
        "h_dim= 1024\n",
        "\n",
        "\n",
        "class conv_VAE(nn.Module):\n",
        "    def __init__(self, nz=32):\n",
        "        super(conv_VAE, self).__init__()\n",
        "        \n",
        "        self.have_cuda = True\n",
        "        self.nz = nz\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            # input is (nc) x 28 x 28\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf * 4, h_dim, 4, 1, 0, bias=False),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( h_dim, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     nc, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(ngf),\n",
        "            # nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            # nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            # nn.Tanh()\n",
        "            nn.Sigmoid()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(h_dim, 512)\n",
        "        self.fc21 = nn.Linear(512, nz)\n",
        "        self.fc22 = nn.Linear(512, nz)\n",
        "\n",
        "        self.fc3 = nn.Linear(nz, 512)\n",
        "        self.fc4 = nn.Linear(512, h_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def encode(self, x):\n",
        "        conv = self.encoder(x);\n",
        "        # print(\"encode conv\", conv.size())\n",
        "        h1 = self.fc1(conv.view(-1, h_dim))\n",
        "        # print(\"encode h1\", h1.size())\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = self.relu(self.fc3(z))\n",
        "        deconv_input = self.fc4(h3)\n",
        "        # print(\"deconv_input\", deconv_input.size())\n",
        "        deconv_input = deconv_input.view(-1, h_dim, 1, 1)\n",
        "        # print(\"deconv_input\", deconv_input.size())\n",
        "        return self.decoder(deconv_input)\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "#         std = logvar.mul(0.5).exp_()\n",
        "#         if self.have_cuda:\n",
        "#             eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "#         else:\n",
        "#             eps = torch.FloatTensor(std.size()).normal_()\n",
        "#         eps = Variable(eps)\n",
        "#         return eps.mul(std).add_(mu)\n",
        "    \n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"x\", x.size())\n",
        "        mu, logvar = self.encode(x)\n",
        "        # print(\"mu, logvar\", mu.size(), logvar.size())\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        # print(\"z\", z.size())\n",
        "        decoded = self.decode(z)\n",
        "        # print(\"decoded\", decoded.size())\n",
        "        return decoded, mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RZ3GUPZC1YGp",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnV_y_vB2tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_valid, x_test = get_binmnist_datasets(root)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n",
        "test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9QQdOltB2tz",
        "colab": {}
      },
      "source": [
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)\n",
        "\n",
        "    return BCE + KLD, kl.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "etfEFnhXB2t2",
        "colab": {}
      },
      "source": [
        "model = conv_VAE().to(device)\n",
        "# model = VAE().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loss_all = []\n",
        "test_loss_all = []\n",
        "kl_loss_train = []\n",
        "kl_loss_test = []\n",
        "test_loss_all = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    batch_idx = 0\n",
        "    batch_kl = []\n",
        "    batch_elbo = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_kl.append(kld.item())\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "            \n",
        "        batch_idx = batch_idx + 1\n",
        "\n",
        "    kl_loss_train.append(np.mean(batch_kl))\n",
        "\n",
        "    train_mean = train_loss / len(train_loader.dataset)\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_mean))\n",
        "    train_loss_all.append(train_mean)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    show_img = False\n",
        "    datapath = os.path.join(root, 'results')\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    batch_kl = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader: \n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss, kld = loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            batch_kl.append(kld.item())\n",
        "\n",
        "            # if i == 0:\n",
        "            #     n = min(data.size(0), 8)\n",
        "            #     comparison = torch.cat([data[:n],\n",
        "            #                           recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "            #     save_image(comparison.cpu(),\n",
        "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "     \n",
        "            recon_batch = recon_batch.to(\"cpu\")\n",
        "\n",
        "            if show_img:\n",
        "                # Show input digits\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(data[i].view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Inputs')\n",
        "                plt.show()\n",
        "\n",
        "                # Show reconstructions\n",
        "                f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                for i, ax in enumerate(axarr.flat):\n",
        "                    ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n",
        "                    ax.axis('off')\n",
        "                plt.suptitle('Reconstructions')\n",
        "                plt.show()\n",
        "\n",
        "                # Show latent space samples        \n",
        "                with torch.no_grad():\n",
        "                    sample = torch.randn(64, 20).to(device)\n",
        "                    sample = model.decode(sample).cpu()\n",
        "                    f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n",
        "                    for i, ax in enumerate(axarr.flat):\n",
        "                        ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n",
        "                        ax.axis('off')\n",
        "                    plt.suptitle('Latent space')\n",
        "                    plt.show()\n",
        "                show_img = False\n",
        "\n",
        "    kl_loss_test.append(np.mean(batch_kl))\n",
        "\n",
        "    test_mean = test_loss / len(test_loader.dataset)\n",
        "    test_loss_all.append(test_mean)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nomD1zG6g_kp",
        "colab": {}
      },
      "source": [
        "def plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test):\n",
        "        # Overall loss (ELBO)\n",
        "        plt.plot(epoch_list, train_loss_all, color=\"blue\")\n",
        "        plt.plot(epoch_list, test_loss_all, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "        # KL loss\n",
        "        plt.plot(epoch_list, kl_loss_train, color=\"blue\")\n",
        "        plt.plot(epoch_list, kl_loss_test, color=\"green\", linestyle=\"--\")\n",
        "        plt.legend(['Training', 'Testing'])\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('KL loss')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1EoPFjWr95NZ",
        "outputId": "8348541c-cf09-4434-a9a3-7e8f1849460d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    epoch_list = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        epoch_list.append(epoch)\n",
        "        if epoch == 1:\n",
        "            continue\n",
        "\n",
        "        # plot_data(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test)\n",
        "        print(\"################################################################\")\n",
        "            \n",
        "            # save_image(sample.view(64, 1, 28, 28),\n",
        "            #            'results/sample_' + str(epoch) + '.png')\n",
        "\n",
        "        #clear_output(wait=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([64, 784])) that is different to the input size (torch.Size([64, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 773.869629\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 190.378220\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 169.958847\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 133.410873\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 124.195358\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 126.558830\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 115.550240\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 106.597588\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 108.575134\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 116.879074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([32, 784])) that is different to the input size (torch.Size([32, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 Average loss: 141.3747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([16, 784])) that is different to the input size (torch.Size([16, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Test set loss: 112.0961\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 103.418022\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 110.184105\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 118.157227\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 108.008575\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 106.984779\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 105.424973\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 101.491806\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 101.815643\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 106.713654\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 104.597305\n",
            "====> Epoch: 2 Average loss: 106.5019\n",
            "====> Test set loss: 102.8744\n",
            "################################################################\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 99.782578\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 99.076454\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 108.341721\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 101.663612\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 101.887650\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 103.581390\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 101.328026\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 99.018654\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 104.152946\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 98.749962\n",
            "====> Epoch: 3 Average loss: 101.9547\n",
            "====> Test set loss: 100.4553\n",
            "################################################################\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 99.189423\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 96.660309\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 99.895187\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 101.053146\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 98.110382\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 101.354080\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.248856\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 102.613327\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 100.639786\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 94.659546\n",
            "====> Epoch: 4 Average loss: 99.8357\n",
            "====> Test set loss: 98.5157\n",
            "################################################################\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 99.493484\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 97.344681\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 98.521889\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 96.306946\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 96.022812\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 96.575966\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 97.396957\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 98.182457\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 95.292404\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 94.741402\n",
            "====> Epoch: 5 Average loss: 98.2632\n",
            "====> Test set loss: 97.7601\n",
            "################################################################\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 101.448616\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 100.921890\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 97.737938\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 91.955704\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 101.852348\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 88.326012\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 96.613174\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 98.779015\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.959442\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 98.395889\n",
            "====> Epoch: 6 Average loss: 97.1569\n",
            "====> Test set loss: 96.0804\n",
            "################################################################\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 101.162292\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 100.339485\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 93.742706\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 88.195335\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 94.463791\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 97.372337\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 88.361824\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 92.495338\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 99.717896\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 90.409187\n",
            "====> Epoch: 7 Average loss: 96.2731\n",
            "====> Test set loss: 95.8040\n",
            "################################################################\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 93.380653\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 95.877777\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 91.310966\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 96.318710\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 96.784363\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 93.461624\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 92.502777\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 93.120529\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 89.543083\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 91.800018\n",
            "====> Epoch: 8 Average loss: 95.5022\n",
            "====> Test set loss: 96.5184\n",
            "################################################################\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 96.968430\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 96.106689\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 98.875267\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 93.244141\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 101.868027\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 90.417397\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 96.055038\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 93.269485\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 99.107941\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 92.866394\n",
            "====> Epoch: 9 Average loss: 94.8522\n",
            "====> Test set loss: 95.4673\n",
            "################################################################\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 94.367966\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 97.730049\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 88.669647\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 96.926193\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 97.883232\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 90.444084\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 87.275322\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 88.401390\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 89.144341\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 98.875076\n",
            "====> Epoch: 10 Average loss: 94.3560\n",
            "====> Test set loss: 94.4364\n",
            "################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_4qjV5xB2uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}