{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of newVAE_4_working.ipynb","provenance":[{"file_id":"1gKw13YHvIHtaNdC9-r-iKUyZtGOqyBYT","timestamp":1574010359516}],"collapsed_sections":["Su8sZZ7Kaogp"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Su8sZZ7Kaogp","colab_type":"text"},"source":["#Imports"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z3-2CeFd6e3g","colab":{}},"source":["from __future__ import print_function\n","from urllib.request import urlretrieve\n","from torch.utils.data import Dataset\n","from torch import optim\n","\n","import os\n","import pickle as pkl\n","import numpy as np\n","import torch\n","\n","import torch\n","import torch.utils.data\n","\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import os\n","import torch.nn as nn\n","\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrtuuU61atWz","colab_type":"text"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YfRkJJM26-M7","outputId":"f975668c-1ab3-4bdb-b669-e185dcfddcef","executionInfo":{"status":"ok","timestamp":1574014635269,"user_tz":-60,"elapsed":787,"user":{"displayName":"Ionela Marinuta","photoUrl":"","userId":"08311922530825934437"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n","\n","batch_size = 64\n","log_interval = 100\n","epochs = 20\n","use_conv = True\n","bottleneck_dim = 32\n","\n","root = os.getcwd()\n","print(f\"Current working directory: {root}\")"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Current working directory: /content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x-zthwwXa5Rj","colab_type":"text"},"source":["# Load data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yj5MFq1c5CPd","colab":{},"cellView":"both"},"source":["def load_mnist_binarized(root):\n","    datapath = os.path.join(root, 'bin-mnist')\n","    if not os.path.exists(datapath):\n","        os.makedirs(datapath)\n","    dataset = os.path.join(datapath, \"mnist.pkl.gz\")\n","\n","    if not os.path.isfile(dataset):\n","\n","        datafiles = {\n","            \"train\": \"http://www.cs.toronto.edu/~larocheh/public/\"\n","                     \"datasets/binarized_mnist/binarized_mnist_train.amat\",\n","            \"valid\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n","                     \"binarized_mnist/binarized_mnist_valid.amat\",\n","            \"test\": \"http://www.cs.toronto.edu/~larocheh/public/datasets/\"\n","                    \"binarized_mnist/binarized_mnist_test.amat\"\n","        }\n","        datasplits = {}\n","        for split in datafiles.keys():\n","            print(\"Downloading %s data...\" % (split))\n","            datasplits[split] = np.loadtxt(urlretrieve(datafiles[split])[0])\n","\n","        pkl.dump([datasplits['train'], datasplits['valid'], datasplits['test']], open(dataset, \"wb\"))\n","\n","    x_train, x_valid, x_test = pkl.load(open(dataset, \"rb\"))\n","    return x_train, x_valid, x_test\n","\n","\n","class BinMNIST(Dataset):\n","    \"\"\"Binary MNIST dataset\"\"\"\n","\n","    def __init__(self, data, device='cpu', transform=None):\n","        h, w, c = 28, 28, 1\n","        self.device = device\n","        self.data = torch.tensor(data, dtype=torch.float).view(-1, c, h, w)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample.to(self.device)\n","\n","\n","def get_binmnist_datasets(root, device='cpu'):\n","    x_train, x_valid, x_test = load_mnist_binarized(root)\n","    x_train = np.append(x_train, x_valid, axis=0)  # https://github.com/casperkaae/LVAE/blob/master/run_models.py (line 401)\n","    return BinMNIST(x_train, device=device), BinMNIST(x_test, device=device), BinMNIST(x_test, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQImwISM6hL8","colab_type":"code","outputId":"71c9b74a-ef20-4708-e80e-cb2f43156757","executionInfo":{"status":"ok","timestamp":1574012757568,"user_tz":-60,"elapsed":41614,"user":{"displayName":"Ionela Marinuta","photoUrl":"","userId":"08311922530825934437"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["x_train, x_valid, x_test = get_binmnist_datasets(root)\n","\n","train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True, pin_memory=cuda)\n","test_loader  = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=True, pin_memory=cuda)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Downloading train data...\n","Downloading valid data...\n","Downloading test data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vbgY32oYbM1d","colab_type":"text"},"source":["# VAE"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RZ3GUPZC1YGp","colab":{}},"source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 400)\n","        self.fc21 = nn.Linear(400, 20)\n","        self.fc22 = nn.Linear(400, 20)\n","        self.fc3 = nn.Linear(20, 400)\n","        self.fc4 = nn.Linear(400, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x.view(-1, 784))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1rwxeOLbAFE","colab_type":"text"},"source":["# VAE Conv"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UPaACQn06BbW","colab":{}},"source":["ngf = 4\n","ndf = 4\n","nc = 1\n","h_dim=1024\n","\n","class conv_VAE(nn.Module):\n","    def __init__(self, nz=32):\n","        super(conv_VAE, self).__init__()\n","        \n","        self.nz = nz\n","        \n","        self.encoder = nn.Sequential(\n","            # input is (nc) x 28 x 28\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 14 x 14\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 7 x 7\n","            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 4 x 4\n","            nn.Conv2d(ndf * 4, 1024, 4, 1, 0, bias=False),\n","            # nn.BatchNorm2d(1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # nn.Sigmoid()\n","        )\n","        \n","        \n","        self.decoder = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( 1024, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d(ngf * 2,     nc, 4, 2, 1, bias=False),\n","            # nn.BatchNorm2d(ngf),\n","            # nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            # nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n","            # nn.Tanh()\n","            nn.Sigmoid()\n","            # state size. (nc) x 64 x 64\n","        )\n","        \n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc21 = nn.Linear(512, nz)\n","        self.fc22 = nn.Linear(512, nz)\n","\n","        self.fc3 = nn.Linear(nz, 512)\n","        self.fc4 = nn.Linear(512, 1024)\n","\n","        self.lrelu = nn.LeakyReLU()\n","        self.relu = nn.ReLU()\n","        # self.sigmoid = nn.Sigmoid()\n","        \n","    def encode(self, x):\n","        conv = self.encoder(x);\n","        # print(\"encode conv\", conv.size())\n","        h1 = self.fc1(conv.view(-1, 1024))\n","        # print(\"encode h1\", h1.size())\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def decode(self, z):\n","        h3 = self.relu(self.fc3(z))\n","        deconv_input = self.fc4(h3)\n","        # print(\"deconv_input\", deconv_input.size())\n","        deconv_input = deconv_input.view(-1,1024,1,1)\n","        # print(\"deconv_input\", deconv_input.size())\n","        return self.decoder(deconv_input)\n","\n","    def reparametrize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def forward(self, x):\n","        # print(\"x\", x.size())\n","        mu, logvar = self.encode(x)\n","        # print(\"mu, logvar\", mu.size(), logvar.size())\n","        z = self.reparametrize(mu, logvar)\n","        # print(\"z\", z.size())\n","        decoded = self.decode(z)\n","        # print(\"decoded\", decoded.size())\n","        return decoded, mu, logvar"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wzyby7GAci13","colab_type":"text"},"source":["# Loss function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WHj3L8TX6hMD","colab":{}},"source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)\n","\n","    return BCE + KLD, kl.mean()\n","    # return BCE + KLD, BCE, kl.mean()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pxbzTp-PeoJC","colab_type":"text"},"source":["# Train"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HHRhqae36hMJ","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    batch_loss, batch_kl = [],[]\n","    batch_idx = 0\n","    # batch_elbo = []\n","\n","    for data in train_loader:\n","        data = data.to(device)\n","\n","        recon_batch, mu, logvar = model(data)\n","        loss, kld = loss_function(recon_batch, data, mu, logvar)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        batch_loss.append(loss.item())\n","        batch_kl.append(kld.item())\n","\n","        if batch_idx % log_interval == 0:\n","            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')   \n","        batch_idx = batch_idx + 1\n","\n","    epoch_kl_train.append(np.mean(batch_kl))\n","    epoch_loss_train.append(np.mean(batch_loss))\n","    print('====> Epoch: {epoch} Average loss: {np.mean(batch_loss):.4f}')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7pk6yvBtfP3","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"zS2xG1ugtHCD","colab_type":"code","colab":{}},"source":["def test(epoch):\n","    show_img = False\n","    datapath = os.path.join(root, 'results')\n","    if not os.path.exists(datapath):\n","        os.makedirs(datapath)\n","    \n","    model.eval()\n","    batch_loss, batch_kl = [],[]\n","    \n","    with torch.no_grad():\n","        for data in test_loader: \n","            data = data.to(device)\n","\n","            recon_batch, mu, logvar = model(data)\n","            loss, kld = loss_function(recon_batch, data, mu, logvar)\n","\n","            batch_loss.append(loss.item())\n","            batch_kl.append(kld.item())\n","     \n","            recon_batch = recon_batch.to(\"cpu\")\n","\n","            if show_img:\n","                plot_images(data, recon_batch, 32)\n","                show_img = False\n","\n","    epoch_kl_test.append(np.mean(batch_kl))\n","    epoch_loss_test.append(np.mean(batch_loss))\n","    print('====> Test set loss: {np.mean(batch_loss):.4f}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpHAtrb_e-m9","colab_type":"text"},"source":["# Plots"]},{"cell_type":"code","metadata":{"id":"J1hXlrHTt-bX","colab_type":"code","colab":{}},"source":["import matplotlib.gridspec as gridspec\n","\n","# Plots the input, reconstructions, and latent space samples for \n","def plot_images(data, recon_batch, bottleneck_dim):\n","    \n","    fig = plt.figure(figsize=(3, 9))\n","    outer = gridspec.GridSpec(1, 3, wspace=0.2, hspace=0.2)\n","\n","    for i in range(3):\n","        inner = gridspec.GridSpecFromSubplotSpec(8, 8,\n","                        subplot_spec=outer[i], wspace=0.1, hspace=0.1)\n","        print(inner)\n","        for j in range(2):\n","            # inputs\n","            ax = plt.Subplot(fig, inner[1])\n","            print(ax)\n","            for i, ax in enumerate(ax.ax.flat):\n","                ax.imshow(data[i].cpu().view(28, 28), cmap=\"binary_r\")\n","                ax.axis('off')\n","                ax.suptitle('Inputs')\n","            fig.add_subplot(ax)\n","\n","            # reconstructions\n","            ax = plt.Subplot(fig, inner[1])\n","            for i, ax in enumerate(ax.flat):\n","                ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n","                ax.axis('off')\n","                ax.suptitle('Reconstructions')\n","            fig.add_subplot(ax)\n","\n","            # latent space\n","            ax = plt.Subplot(fig, inner[2])\n","            sample = torch.randn(64, bottleneck_dim).to(device)\n","            print(f'sample.shape {sample.shape}')\n","            sample = model.decode(sample).cpu()\n","            f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n","            for i, ax in enumerate(ax.flat):\n","                ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n","                ax.axis('off')\n","                ax.suptitle('Latent samples')\n","            fig.add_subplot(ax)\n","    fig.show()\n","\n","    # # Show input digits\n","    # f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n","    # for i, ax in enumerate(axarr.flat):\n","    #     ax.imshow(data[i].cpu().view(28, 28), cmap=\"binary_r\")\n","    #     ax.axis('off')\n","    # plt.suptitle('Inputs')\n","    # plt.show()\n","\n","    # # Show reconstructions\n","    # f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n","    # for i, ax in enumerate(axarr.flat):\n","    #     ax.imshow(recon_batch[i].view(28, 28), cmap=\"binary_r\")\n","    #     ax.axis('off')\n","    # plt.suptitle('Reconstructions')\n","    # plt.show()\n","\n","    # # Show latent space samples        \n","    # with torch.no_grad():\n","    #     sample = torch.randn(64, bottleneck_dim).to(device)\n","    #     print(f'sample.shape {sample.shape}')\n","    #     sample = model.decode(sample).cpu()\n","    #     f, axarr = plt.subplots(8, 8, figsize=(8, 8))\n","    #     for i, ax in enumerate(axarr.flat):\n","    #         ax.imshow(sample[i].view(28, 28), cmap=\"binary_r\")\n","    #         ax.axis('off')\n","    #     plt.suptitle('Latent space')\n","    #     plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nomD1zG6g_kp","colab":{}},"source":["def plot_losses(epoch_list, train_loss_all, test_loss_all, kl_loss_train, kl_loss_test):\n","        # Overall loss (ELBO)\n","        print(f'epoch_list {epoch_list}')\n","        print(f'train_loss_all {train_loss_all}')\n","        plt.plot(epoch_list, train_loss_all, color=\"blue\")\n","        plt.plot(epoch_list, test_loss_all, color=\"green\", linestyle=\"--\")\n","        plt.legend(['Training', 'Testing'])\n","        plt.xlabel('epochs')\n","        plt.ylabel('loss')\n","        plt.show()\n","\n","        # KL loss\n","        plt.plot(epoch_list, kl_loss_train, color=\"blue\")\n","        plt.plot(epoch_list, kl_loss_test, color=\"green\", linestyle=\"--\")\n","        plt.legend(['Training', 'Testing'])\n","        plt.xlabel('epochs')\n","        plt.ylabel('KL loss')\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRh8SXJbfC7j","colab_type":"text"},"source":["# Main"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1EoPFjWr95NZ","outputId":"60eaf72e-2caa-46d6-a458-b441bdced264","executionInfo":{"status":"error","timestamp":1574020834220,"user_tz":-60,"elapsed":5822,"user":{"displayName":"Ionela Marinuta","photoUrl":"","userId":"08311922530825934437"}},"colab":{"base_uri":"https://localhost:8080/","height":528}},"source":["model = conv_VAE().to(device) if use_conv else VAE().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","if __name__ == \"__main__\":\n","\n","    epoch_list = []\n","    epoch_loss_train, epoch_kl_train = [], [] # Train losses per epoch (mean of batch losses)\n","    epoch_loss_test, epoch_kl_test = [], [] # Test losses per epoch (mean of batch losses)\n","    \n","    for epoch in range(1, epochs + 1):\n","        train(epoch)\n","        test(epoch)\n","        \n","        epoch_list.append(epoch)\n","        if epoch == 1:\n","            continue\n","\n","        plot_losses(epoch_list, epoch_loss_train, epoch_loss_test, epoch_kl_train, epoch_kl_test)\n","        print(\"################################################################\")\n","            \n","        # save_image(sample.view(64, 1, 28, 28),\n","        #            'results/sample_' + str(epoch) + '.png')\n","        #clear_output(wait=True)\n"],"execution_count":132,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([64, 784])) that is different to the input size (torch.Size([64, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 576.400330\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 271.281006\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 205.469788\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 180.509781\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-132-36fc43789d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-96-618410ebba00>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}